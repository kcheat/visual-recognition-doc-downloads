{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a826726-3cde-4bb8-94e8-213ee1e34993",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo: Model Training and Inference with NLU Classifications\n",
    "\n",
    "In this notebook, we'll be looking at the most recent feature released in IBM Watson Natural Language Understanding (NLU): **The option to train custom single-label vs. multi-label models**. In addition, we'll go through a couple of *best practices for getting the most out of the model predictions according to individual use cases*.\n",
    "\n",
    "For a broad overview of training a custom classification model using NLU, see this [demo](https://github.com/watson-developer-cloud/doc-tutorial-downloads/blob/master/natural-language-understanding/custom_classifications_example.ipynb).\n",
    "\n",
    "Most of our setup is taken from that demo notebook, and we'll have duplicates of each cell to differentiate single-label and multi-label models. This means that we'll create two NLU instances - but this is only for demo purposes since we can only have one custom model at a time in the NLU free tier - you can choose to run only the cells related to single-label or multi-label models.\n",
    "\n",
    "#### Requirements:\n",
    "- `ibm_watson`\n",
    "- `sklearn`\n",
    "- `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d3495-ee3a-4291-ad6a-afde0dd49723",
   "metadata": {},
   "source": [
    "## 0. Install requirements\n",
    "\n",
    "If you don't have the libraries required, uncomment the following cell to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfcaeaf2-be0a-4f80-a6d3-9eb83d5bb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ibm_watson\n",
    "# !pip install sklearn\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe7928-6e31-4740-9be5-275216287362",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 1. Setup the NLU Service\n",
    "\n",
    "\n",
    "See the following for authenticating to Watson services: https://cloud.ibm.com/docs/watson?topic=watson-iam. It will suffice to use the auto-generated service credentials when you instantiated the NLU service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db251200-a2ec-45c5-99b0-c7931b9c3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b872189-a57c-4520-b658-7db0c0c00785",
   "metadata": {},
   "source": [
    "#### Single-label setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f77d0421-4139-4725-911e-59d73bbe8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your NLU credentials here\n",
    "single_api_key = \"[INSERT YOUR API KEY HERE]\"\n",
    "single_url = \"[INSERT YOUR NLU URL HERE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e6ad7c5-8792-4af7-b76f-a460996fc999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected with the NLU service for our single-label model\n"
     ]
    }
   ],
   "source": [
    "single_auth = IAMAuthenticator(single_api_key)\n",
    "single_nlu = NaturalLanguageUnderstandingV1(version='2022-08-10', authenticator=single_auth)\n",
    "single_nlu.set_service_url(single_url)\n",
    "\n",
    "print(\"Successfully connected with the NLU service for our single-label model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed665706-27fd-4127-8144-e71f61e6584d",
   "metadata": {},
   "source": [
    "#### Multi-label setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9b4c013-273a-4095-868a-bddeb56c4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your NLU credentials here\n",
    "multi_api_key = \"[INSERT YOUR API KEY HERE]\"\n",
    "multi_url = \"[INSERT YOUR NLU URL HERE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22c095ce-9379-45c6-9e98-e879ecfc18c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected with the NLU service for our multi-label model\n"
     ]
    }
   ],
   "source": [
    "multi_auth = IAMAuthenticator(multi_api_key)\n",
    "multi_nlu = NaturalLanguageUnderstandingV1(version='2022-08-10', authenticator=multi_auth)\n",
    "multi_nlu.set_service_url(multi_url)\n",
    "\n",
    "print(\"Successfully connected with the NLU service for our multi-label model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8badc-1de8-48fd-a36b-10fdff80230a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Creating Training Data\n",
    "\n",
    "Before creating our training data, let's define `single-label` vs. `multi-label`. \n",
    "- **Single-label**: Each example in the dataset can have *only one label*:\n",
    "\n",
    "```\n",
    "{\n",
    "        \"text\": \"How hot is it today?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "}\n",
    "```\n",
    "\n",
    "- **Multi-label**: Each example in the dataset can have *more than one label*:\n",
    "\n",
    "```\n",
    "{\n",
    "        \"text\": \"How hot is it today?\",\n",
    "        \"labels\": [\"temperature\", \"question\", \"assistance\"]\n",
    "}\n",
    "```\n",
    "\n",
    "If all of the data examples in our dataset are single-label, then we'd need to train a single-label model. Conversely, if we expect multiple labels per each example, then we'd train a multi-label model. Notice how the data format for input doesn't change regardless of whether the dataset is multi-label or single-label.\n",
    "\n",
    "For our demo, we'll use a single-label dataset with three labels in total, one of which has less data examples than the other two, to denote a real case of an *imbalanced dataset*. This generally occurs when we only have a small set of data for a type of examples that doesn't occur often, such as extreme weather conditions.\n",
    "\n",
    "For our toy dataset, we'll have:\n",
    "- 8 examples for the label `temperature`\n",
    "- 8 examples for the label `conditions`\n",
    "- 5 examples for the label `emergencies`\n",
    "\n",
    "\n",
    "**NOTE: A minimum of 5 examples per class is required to train a model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32ff674a-a9f4-4f80-a176-f470371c627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"text\": \"How hot is it today?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it hot outside?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Will it be uncomfortably hot?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Will it be sweltering?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"How cold is it today?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"What's the real-feel?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it freezing?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it warm outside?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Will we get snow?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Are we expecting sunny conditions?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it overcast?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Will it be cloudy?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Will there be hail tomorrow?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Will there be a blizzard tonight?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it snowing right now?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it going to rain?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Has there been a crash?\",\n",
    "        \"labels\": [\"emergencies\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is there a wildfire?\",\n",
    "        \"labels\": [\"emergencies\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Are the roads blocked?\",\n",
    "        \"labels\": [\"emergencies\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is someone missing?\",\n",
    "        \"labels\": [\"emergencies\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I need help!\",\n",
    "        \"labels\": [\"emergencies\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee9cfc5-f043-4880-af33-b3c8e5f41bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved locally in training_data.json\n"
     ]
    }
   ],
   "source": [
    "# Save Training data in a file\n",
    "import json\n",
    "\n",
    "training_data_filename = 'training_data.json'\n",
    "\n",
    "with open(training_data_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, indent=4)\n",
    "\n",
    "print('Data successfully saved locally in ' + training_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6f22c-acad-470b-85f9-0b168a17dab5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 3. How to Train a NLU Classifications Model: Single-label vs. Multi-label\n",
    "\n",
    "To train a NLU Classifications model using the data created above, utilize the `create_classifications_model` method. To specify whether the model is single-label or multi-label, you can pass a dictionary with a `model_type` to the `training_parameters` argument.\n",
    "\n",
    "- To create a single-label model:\n",
    "```\n",
    "nlu.create_classifications_model(...,\n",
    "                                training_parameters={\"model_type\": \"single_label\"},\n",
    "                                ...)\n",
    "```\n",
    "\n",
    "- To create a multi-label model:\n",
    "```\n",
    "nlu.create_classifications_model(...,\n",
    "                                training_parameters={\"model_type\": \"multi_label\"},\n",
    "                                ...)\n",
    "```\n",
    "\n",
    "\n",
    "**NOTE: This cell will start a training job for the model and return the model information, but the training will continue even if the cell has finished running. To check the status of the model, run the cell below `Checking the status of the models` and look at the `status` key in the model information.**\n",
    "\n",
    "\n",
    "To view all functionality, you can also look over the NLU API documentation: https://cloud.ibm.com/apidocs/natural-language-understanding?code=python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3bff83-6108-4e0f-9db9-e2db71153ef5",
   "metadata": {},
   "source": [
    "### Single-label training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d0fdb29-6d38-4b82-8d8b-f1e3b4cc2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a NLU Single Label Classifications model:\n",
      "{\n",
      "    \"name\": \"MySingleLabelClassificationsModel\",\n",
      "    \"user_metadata\": null,\n",
      "    \"language\": \"en\",\n",
      "    \"description\": null,\n",
      "    \"model_version\": \"1.0.1\",\n",
      "    \"version\": \"1.0.1\",\n",
      "    \"workspace_id\": null,\n",
      "    \"version_description\": null,\n",
      "    \"status\": \"starting\",\n",
      "    \"notices\": [],\n",
      "    \"model_id\": \"16d9969c-232f-437e-96de-5436b867b366\",\n",
      "    \"features\": [\n",
      "        \"classifications\"\n",
      "    ],\n",
      "    \"created\": \"2022-08-23T17:18:15Z\",\n",
      "    \"last_trained\": \"2022-08-23T17:18:15Z\",\n",
      "    \"last_deployed\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(training_data_filename, 'r') as file:\n",
    "    single_label_model = single_nlu.create_classifications_model(language='en', \n",
    "                                                          training_data=file, \n",
    "                                                          training_parameters={\"model_type\": \"single_label\"}, \n",
    "                                                          training_data_content_type='application/json', \n",
    "                                                          name='MySingleLabelClassificationsModel', model_version='1.0.1').get_result()\n",
    "\n",
    "    print(\"Created a NLU Single Label Classifications model:\")\n",
    "    print(json.dumps(single_label_model, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ae3a9-1c8d-486a-8f17-9e769020bda7",
   "metadata": {},
   "source": [
    "### Multi-label training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b65cc799-97f5-4482-b195-35c43acb4042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a NLU Multi Label Classifications model:\n",
      "{\n",
      "    \"name\": \"MyMultiLabelClassificationsModel\",\n",
      "    \"user_metadata\": null,\n",
      "    \"language\": \"en\",\n",
      "    \"description\": null,\n",
      "    \"model_version\": \"1.0.1\",\n",
      "    \"version\": \"1.0.1\",\n",
      "    \"workspace_id\": null,\n",
      "    \"version_description\": null,\n",
      "    \"status\": \"starting\",\n",
      "    \"notices\": [],\n",
      "    \"model_id\": \"d014a8a4-c566-49bf-8a64-635bdb42e4a7\",\n",
      "    \"features\": [\n",
      "        \"classifications\"\n",
      "    ],\n",
      "    \"created\": \"2022-08-23T17:18:16Z\",\n",
      "    \"last_trained\": \"2022-08-23T17:18:16Z\",\n",
      "    \"last_deployed\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(training_data_filename, 'r') as file:\n",
    "    multi_label_model = multi_nlu.create_classifications_model(language='en', \n",
    "                                                          training_data=file, \n",
    "                                                          training_parameters={\"model_type\": \"multi_label\"}, \n",
    "                                                          training_data_content_type='application/json', \n",
    "                                                          name='MyMultiLabelClassificationsModel', model_version='1.0.1').get_result()\n",
    "    print(\"Created a NLU Multi Label Classifications model:\")\n",
    "    print(json.dumps(multi_label_model, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cb3c0-5a04-4a1c-8105-aa287889e238",
   "metadata": {},
   "source": [
    "### Checking the status of the models\n",
    "\n",
    "When the model is training, the value of `status` will be `training`. When the model is done training and ready to use, the value of `status` will be `available`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ebd073-ed0a-441a-8a9b-34457977a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about the created Single-label NLU Classifications model:\n",
      "{\n",
      "    \"name\": \"MySingleLabelClassificationsModel\",\n",
      "    \"user_metadata\": null,\n",
      "    \"language\": \"en\",\n",
      "    \"description\": null,\n",
      "    \"model_version\": \"1.0.1\",\n",
      "    \"version\": \"1.0.1\",\n",
      "    \"workspace_id\": null,\n",
      "    \"version_description\": null,\n",
      "    \"status\": \"available\",\n",
      "    \"notices\": [],\n",
      "    \"model_id\": \"16d9969c-232f-437e-96de-5436b867b366\",\n",
      "    \"features\": [\n",
      "        \"classifications\"\n",
      "    ],\n",
      "    \"created\": \"2022-08-23T17:18:15Z\",\n",
      "    \"last_trained\": \"2022-08-23T17:18:15Z\",\n",
      "    \"last_deployed\": \"2022-08-23T17:25:11Z\"\n",
      "}\n",
      "Information about the created Multi-label NLU Classifications model:\n",
      "{\n",
      "    \"name\": \"MyMultiLabelClassificationsModel\",\n",
      "    \"user_metadata\": null,\n",
      "    \"language\": \"en\",\n",
      "    \"description\": null,\n",
      "    \"model_version\": \"1.0.1\",\n",
      "    \"version\": \"1.0.1\",\n",
      "    \"workspace_id\": null,\n",
      "    \"version_description\": null,\n",
      "    \"status\": \"available\",\n",
      "    \"notices\": [],\n",
      "    \"model_id\": \"d014a8a4-c566-49bf-8a64-635bdb42e4a7\",\n",
      "    \"features\": [\n",
      "        \"classifications\"\n",
      "    ],\n",
      "    \"created\": \"2022-08-23T17:18:16Z\",\n",
      "    \"last_trained\": \"2022-08-23T17:18:16Z\",\n",
      "    \"last_deployed\": \"2022-08-23T17:28:11Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "single_model_id = single_label_model['model_id']\n",
    "single_model_to_view = single_nlu.get_classifications_model(model_id=single_model_id).get_result()\n",
    "multi_model_id = multi_label_model['model_id']\n",
    "multi_model_to_view = multi_nlu.get_classifications_model(model_id=multi_model_id).get_result()\n",
    "\n",
    "print(\"Information about the created Single-label NLU Classifications model:\")\n",
    "print(json.dumps(single_model_to_view, indent=4))\n",
    "print(\"Information about the created Multi-label NLU Classifications model:\")\n",
    "print(json.dumps(multi_model_to_view, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fad4b-dd41-49b3-bad7-1bc2c1790903",
   "metadata": {},
   "source": [
    "## 4. How to Use a Trained NLU Classifications Model for Analysis\n",
    "\n",
    "Once the NLU Classifications model is fully trained, the `status` located in the cell above will turn to `available` indicating the model can be used for analysis (training above will take a few minutes to complete). Once ready, utilize the `analyze` method by passing in text, HTML, or public webpage urls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ff5050c-44f7-4726-86ae-6f07e993abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson.natural_language_understanding_v1 import Features, ClassificationsOptions\n",
    "\n",
    "text = \"is there lightning today?\" #\"What is the expected high for today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a50325-3a76-4e17-98ab-fe22b6c5c0b5",
   "metadata": {},
   "source": [
    "### Single-label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b58c885c-9be8-4a05-a6cc-dd1bf6b8ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis response from trained Single Label NLU Classifications model:\n",
      "{\n",
      "    \"usage\": {\n",
      "        \"text_units\": 1,\n",
      "        \"text_characters\": 25,\n",
      "        \"features\": 1\n",
      "    },\n",
      "    \"language\": \"en\",\n",
      "    \"classifications\": [\n",
      "        {\n",
      "            \"confidence\": 0.441537,\n",
      "            \"class_name\": \"conditions\"\n",
      "        },\n",
      "        {\n",
      "            \"confidence\": 0.418804,\n",
      "            \"class_name\": \"temperature\"\n",
      "        },\n",
      "        {\n",
      "            \"confidence\": 0.139659,\n",
      "            \"class_name\": \"emergencies\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "single_pred = single_nlu.analyze(text=text, features=Features(classifications=ClassificationsOptions(model=single_model_id))).get_result()\n",
    "\n",
    "print(\"Analysis response from trained Single Label NLU Classifications model:\")\n",
    "print(json.dumps(single_pred, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c74702-9e43-4a1f-b885-956f1d36535d",
   "metadata": {},
   "source": [
    "As we can see, the confidence scores are normalized, meaning that they will add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca047e0-6445-4519-a3f3-9a0fa1c81937",
   "metadata": {},
   "source": [
    "### Multi-label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0796ac57-f565-415b-8655-f840f356f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis response from trained Multi Label NLU Classifications model:\n",
      "{\n",
      "    \"usage\": {\n",
      "        \"text_units\": 1,\n",
      "        \"text_characters\": 25,\n",
      "        \"features\": 1\n",
      "    },\n",
      "    \"language\": \"en\",\n",
      "    \"classifications\": [\n",
      "        {\n",
      "            \"confidence\": 0.391887,\n",
      "            \"class_name\": \"conditions\"\n",
      "        },\n",
      "        {\n",
      "            \"confidence\": 0.357356,\n",
      "            \"class_name\": \"temperature\"\n",
      "        },\n",
      "        {\n",
      "            \"confidence\": 0.169718,\n",
      "            \"class_name\": \"emergencies\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "multi_pred = multi_nlu.analyze(text=text, features=Features(classifications=ClassificationsOptions(model=multi_model_id))).get_result()\n",
    "\n",
    "print(\"Analysis response from trained Multi Label NLU Classifications model:\")\n",
    "print(json.dumps(multi_pred, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68507d40-432f-49ef-9682-41014acf16ac",
   "metadata": {},
   "source": [
    "In the multi-label case, we can see that each score represents how aligned the text is with each label independently of each other, and therefore the scores don't add up to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c1578-bbc4-4ced-aaa7-592033281fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. How to select a good prediction threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d25003-e7aa-489d-8895-5fdf20f4e87c",
   "metadata": {},
   "source": [
    "We managed to get predictions from our models - great! But they only give us a confidence score for each potential label. *The final prediction is made by the user.*\n",
    "\n",
    "In the single label-case, we can just choose the class with the highest confidence score/probability, but what happens in the multi-label case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bdb57-d1b1-4b4c-818b-a0af581867b7",
   "metadata": {},
   "source": [
    "### Multi-label final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddef016-4465-46f5-a873-57b46d0d0783",
   "metadata": {},
   "source": [
    "Looking at the multi-label model predictions, we could take the labels that have a confidence score above a given threshold, say `0.33` (so we'd take `conditions` and `temperature`, and discard the other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce17c08a-d0da-4311-9fe2-35119c51663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 0.391887, 'class_name': 'conditions'},\n",
       " {'confidence': 0.357356, 'class_name': 'temperature'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.33\n",
    "multi_final_pred = [pred for pred in multi_pred[\"classifications\"] if pred[\"confidence\"] > threshold]\n",
    "multi_final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f4f56-bf89-412b-8f53-7a97543e47c4",
   "metadata": {},
   "source": [
    "But what if we end up having too many unwanted labels per example (false positives) - and in this case, what if the right answer was **just** `conditions` instead of `conditions` and `temperature`?\n",
    "\n",
    "\n",
    "In this case, we can *increase* the confidence threshold to a number that yields more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3312c1c2-a715-4f80-9a50-dd05f1098fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 0.391887, 'class_name': 'conditions'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.39\n",
    "multi_final_pred = [pred for pred in multi_pred[\"classifications\"] if pred[\"confidence\"] > threshold]\n",
    "multi_final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73a7f2-a17c-49af-949b-bfc2269c4962",
   "metadata": {},
   "source": [
    "### Choosing the right threshold\n",
    "\n",
    "So how do we choose the right prediction threshold? By exploring multiple potential thresholds within a range and choosing the one that **produces the highest score on the *test set* for a metric of our choosing.**\n",
    "\n",
    "For example, let's choose the `micro f1-score` as a metric for our case, and tackle the multi-label case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b68e3c-53e8-486d-bfea-c3b31884fcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806129e1-b578-4b8f-b4b3-ebc78c8239a3",
   "metadata": {},
   "source": [
    "In `sklearn`, there are different types of `f1-scores` we can chose from (more on that in their [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)). We can choose between `weighted`, `micro` and `macro` f1 scores. For our case, we'll choose the `micro` f1 score because it calculates the metric globally across all the potential labels/classes without favoring the performance on any class in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d01a6-fa3e-4288-8774-e46a95c0fd84",
   "metadata": {},
   "source": [
    "#### Test set\n",
    "We'll create some dummy test data and dummy test predictions - note that the relationship between text and labels may not necessarily make too much sense, this is for demonstration purposes. Imagine we are trying to classify whether our input text should be labeled as `temperature`, `conditions` or `emergencies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6eee960a-37d2-47b8-bbc7-350e70d28372",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"text\": \"Is it dry or is it drizzling today?\",\n",
    "        \"labels\": [\"temperature\", \"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Should I wear pants or shorts?\",\n",
    "        \"labels\": [\"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it hailing outside?\",\n",
    "        \"labels\": [\"conditions\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Is it humid and cold and raining?\",\n",
    "        \"labels\": [\"conditions\", \"temperature\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Did you hear about the accident?\",\n",
    "        \"labels\": [\"emergencies\"]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Are there any survivors from the storm?\",\n",
    "        \"labels\": [\"emergencies\", \"conditions\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "test_labels = [data[\"labels\"] for data in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3b2a1-e5f1-46ab-a0e4-ded0e4217a67",
   "metadata": {},
   "source": [
    "We'll generate predictions for the test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6379c601-e00f-4b34-9c96-8dbe155dcc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usage': {'text_units': 1, 'text_characters': 35, 'features': 1},\n",
       " 'language': 'en',\n",
       " 'classifications': [{'confidence': 0.543259, 'class_name': 'temperature'},\n",
       "  {'confidence': 0.352541, 'class_name': 'conditions'},\n",
       "  {'confidence': 0.029271, 'class_name': 'emergencies'}]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = [multi_nlu.analyze(text=text[\"text\"], features=Features(classifications=ClassificationsOptions(model=multi_model_id))).get_result() for text in test_data]\n",
    "test_preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e2be5-ca96-4a64-b890-2f016c663ba6",
   "metadata": {},
   "source": [
    "... And we'll define a method that calculates our final predictions given model confidence scores and a threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ceb4a5f5-f31e-4b34-ab34-08a3f777647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_predictions(model_preds, threshold):\n",
    "    \"\"\"Given a set of probabilities/confidence scores output by our model, return the final predicted labels\n",
    "    that have a confidence score above a given threshold.\n",
    "    \"\"\"  \n",
    "    # Extract the class name and confidence score from the prediction object \n",
    "    model_preds = [pred[\"classifications\"] for pred in model_preds]\n",
    "    \n",
    "    # Only keep the predictions above a threshold\n",
    "    model_preds = [[pred_obj for pred_obj in pred_obj_list if pred_obj[\"confidence\"] > threshold] for pred_obj_list in model_preds]\n",
    "    \n",
    "    # Extract the class names\n",
    "    final_preds = [[pred_obj[\"class_name\"] for pred_obj in pred_obj_list] for pred_obj_list in model_preds]\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f9ff6-6828-4c29-8a8f-cae1329bb648",
   "metadata": {},
   "source": [
    "... so that we can get to our final predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d85e6103-f78d-46a7-97b6-12c173977684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['temperature', 'conditions'],\n",
       " [],\n",
       " ['temperature'],\n",
       " ['temperature', 'conditions'],\n",
       " ['emergencies'],\n",
       " ['emergencies']]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.33\n",
    "final_preds = compute_final_predictions(test_preds, threshold)\n",
    "final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a3f25-2b69-4070-8520-63f338e99604",
   "metadata": {},
   "source": [
    "A key detail is that `sklearn` expects multi-label predictions (and labels) to be in a matrix of `0`'s and `1`'s, so we'll do a final transformation of the outputs using `MultiLabelBinarizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "554308be-ffd5-4610-983e-299e0b0e749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "label_names = [[\"temperature\", \"conditions\", \"emergencies\"]]\n",
    "MLB = MultiLabelBinarizer().fit(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3d2c3a00-3415-4eeb-9fe5-bb556dfd8e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform both our true labels and the model predictions\n",
    "y_true = MLB.transform(test_labels)\n",
    "y_pred = MLB.transform(final_preds)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68760f9f-0dc0-4c27-b0f6-a0c7fbf754d4",
   "metadata": {},
   "source": [
    "For a threshold of `0.33`, our `micro f1-score` is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4012443e-e489-4005-ba57-01fef824cf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1b239-2122-4150-ab1d-afce89505b76",
   "metadata": {},
   "source": [
    "Great - now let's try to see if we can find a better threshold that gives us a higher `f1-score`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d7d97aec-f03e-4042-8051-209128d29b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def compute_f1_with_threshold(threshold, test_labels, test_preds):\n",
    "    \"\"\"Compute the final predictions given model confidence scores, then use them to calculate the f1 score\n",
    "    \"\"\"\n",
    "    final_preds = compute_final_predictions(test_preds, threshold)\n",
    "    y_pred = MLB.transform(final_preds)\n",
    "    f1_with_threshold = f1_score(test_labels, y_pred, average=\"micro\")\n",
    "    return f1_with_threshold\n",
    "\n",
    "def compute_optimal_threshold(metric_func, test_labels, test_preds):    \n",
    "    \"\"\"Compute an optimal threshold that maximizes a given metric function (such as the f1-score), \n",
    "        given a set of test labels and model confidence scores\n",
    "    \"\"\"\n",
    "    eval_f1_func = partial(metric_func, test_labels=test_labels, test_preds=test_preds)\n",
    "\n",
    "    print(\"Evaluating 1001 threshold values between 0 and 1...\")\n",
    "    vals = map(eval_f1_func, np.linspace(0, 1, 1001))\n",
    "    results = list(zip(np.linspace(0, 1, 1001), vals))\n",
    "    opt_thresh = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    print(f'Found an optimal threshold that maximized our metric!: {opt_thresh[0][0]}')\n",
    "    return opt_thresh[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2db587dd-fbcc-437e-917d-e0e19d55ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1001 threshold values between 0 and 1...\n",
      "Found an optimal threshold that maximized our metric!: 0.225\n"
     ]
    }
   ],
   "source": [
    "opt_thresh = compute_optimal_threshold(compute_f1_with_threshold, y_true, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaf9af-1ba3-4d92-b4d6-ceb39b6d3504",
   "metadata": {},
   "source": [
    "It looks like the threshold that maximizes the `f1-score` for our test set and model probabilities is `0.225`. Let's give that one a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d7a1e5ed-4997-4ced-8071-da1b5ad9a6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds_w_opt_thresh = compute_final_predictions(test_preds, opt_thresh)\n",
    "final_preds_w_opt_thresh = MLB.transform(final_preds_w_opt_thresh)\n",
    "final_preds_w_opt_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ab99e3b6-1cc4-412a-bc10-e7b7037b1a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421052631578948"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, final_preds_w_opt_thresh, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf49ef-8f39-4b5d-8501-cb52b71a603c",
   "metadata": {},
   "source": [
    "Nice! As we can see, our `f1-score` is way better when our threshold is `0.225` than if we had chosen our initial `0.33` threshold. A similar logic can be applied to the single-label case for binary classification (choosing 1 label between 2 classes), which is left as an exercise to the reader. Happy training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wnlp",
   "language": "python",
   "name": "wnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
